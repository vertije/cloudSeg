{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "great-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "classified-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDataset(Dataset):\n",
    "    def __init__(self, r_dir, g_dir, b_dir, nir_dir, gt_dir, pytorch=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Loop through the files in red folder and combine, into a dictionary, the other bands\n",
    "        self.files = [self.combine_files(f, g_dir, b_dir, nir_dir, gt_dir) for f in r_dir.iterdir() if not f.is_dir()]\n",
    "        self.pytorch = pytorch\n",
    "        \n",
    "    def combine_files(self, r_file: Path, g_dir, b_dir,nir_dir, gt_dir):\n",
    "        \n",
    "        files = {'red': r_file, \n",
    "                 'green':g_dir/r_file.name.replace('red', 'green'),\n",
    "                 'blue': b_dir/r_file.name.replace('red', 'blue'), \n",
    "                 'nir': nir_dir/r_file.name.replace('red', 'nir'),\n",
    "                 'gt': gt_dir/r_file.name.replace('red', 'gt')}\n",
    "\n",
    "        return files\n",
    "                                       \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.files)\n",
    "     \n",
    "    def open_as_array(self, idx, invert=False, include_nir=False):\n",
    "\n",
    "        raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),\n",
    "                            np.array(Image.open(self.files[idx]['green'])),\n",
    "                            np.array(Image.open(self.files[idx]['blue'])),\n",
    "                           ], axis=2)\n",
    "    \n",
    "        if include_nir:\n",
    "            nir = np.expand_dims(np.array(Image.open(self.files[idx]['nir'])), 2)\n",
    "            raw_rgb = np.concatenate([raw_rgb, nir], axis=2)\n",
    "    \n",
    "        if invert:\n",
    "            raw_rgb = raw_rgb.transpose((2,0,1))\n",
    "    \n",
    "        # normalize\n",
    "        return (raw_rgb / np.iinfo(raw_rgb.dtype).max)\n",
    "    \n",
    "\n",
    "    def open_mask(self, idx, add_dims=False):\n",
    "        \n",
    "        raw_mask = np.array(Image.open(self.files[idx]['gt']))\n",
    "        raw_mask = np.where(raw_mask==255, 1, 0)\n",
    "        \n",
    "        return np.expand_dims(raw_mask, 0) if add_dims else raw_mask\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = torch.tensor(self.open_as_array(idx, invert=self.pytorch, include_nir=True), dtype=torch.float32)\n",
    "        y = torch.tensor(self.open_mask(idx, add_dims=False), dtype=torch.torch.int64)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    def open_as_pil(self, idx):\n",
    "        \n",
    "        arr = 256*self.open_as_array(idx)\n",
    "        \n",
    "        return Image.fromarray(arr.astype(np.uint8), 'RGB')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = 'Dataset class with {} files'.format(self.__len__())\n",
    "\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vanilla-jungle",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path('38-Cloud_training');\n",
    "data = CloudDataset(base_path/'train_red', \n",
    "                    base_path/'train_green', \n",
    "                    base_path/'train_blue', \n",
    "                    base_path/'train_nir', \n",
    "                    base_path/'train_gt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-federal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 4, 384, 384]), torch.Size([12, 384, 384]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, valid_ds = torch.utils.data.random_split(data, (6000,2400))\n",
    "train_dl = DataLoader(train_ds, batch_size=12, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=12, shuffle=True)\n",
    "\n",
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "activated-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n",
    "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "\n",
    "        self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64*2, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32*2, out_channels, 3, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "\n",
    "        upconv3 = self.upconv3(conv3)\n",
    "\n",
    "        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "        return upconv1\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
    "                            )\n",
    "        return expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "thousand-bumper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2, 384, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet = UNET(4,2)\n",
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape\n",
    "pred = unet(xb)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "metallic-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n",
    "    start = time.time()\n",
    "    model.cuda()\n",
    "\n",
    "    train_loss, valid_loss = [], []\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set trainind mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "\n",
    "            step = 0\n",
    "\n",
    "            # iterate over data\n",
    "            for x, y in dataloader:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "                step += 1\n",
    "\n",
    "                # forward pass\n",
    "                if phase == 'train':\n",
    "                    # zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(x)\n",
    "                    loss = loss_fn(outputs, y)\n",
    "\n",
    "                    # the backward pass frees the graph memory, so there is no \n",
    "                    # need for torch.no_grad in this training pass\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    # scheduler.step()\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(x)\n",
    "                        loss = loss_fn(outputs, y.long())\n",
    "\n",
    "                # stats - whatever is the phase\n",
    "                acc = acc_fn(outputs, y)\n",
    "\n",
    "                running_acc  += acc*dataloader.batch_size\n",
    "                running_loss += loss*dataloader.batch_size \n",
    "\n",
    "                if step % 10 == 0:\n",
    "                    # clear_output(wait=True)\n",
    "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n",
    "                    # print(torch.cuda.memory_summary())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_acc / len(dataloader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return train_loss, valid_loss    \n",
    "\n",
    "def acc_metric(predb, yb):\n",
    "    return (predb.argmax(dim=1) == yb.cuda()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tight-liberty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2\n",
      "----------\n",
      "Current step: 10  Loss: 0.5635021924972534  Acc: 0.5792903304100037  AllocMem (Mb): 63.791015625\n",
      "Current step: 20  Loss: 0.5696985721588135  Acc: 0.6491043567657471  AllocMem (Mb): 63.791015625\n",
      "Current step: 30  Loss: 0.3494044542312622  Acc: 0.9052988886833191  AllocMem (Mb): 63.791015625\n",
      "Current step: 40  Loss: 0.37281349301338196  Acc: 0.8047575950622559  AllocMem (Mb): 63.791015625\n",
      "Current step: 50  Loss: 0.36641278862953186  Acc: 0.784929633140564  AllocMem (Mb): 63.791015625\n",
      "Current step: 60  Loss: 0.4390811622142792  Acc: 0.8041235208511353  AllocMem (Mb): 63.791015625\n",
      "Current step: 70  Loss: 0.5288694500923157  Acc: 0.6883183121681213  AllocMem (Mb): 63.791015625\n",
      "Current step: 80  Loss: 0.33339887857437134  Acc: 0.8409034013748169  AllocMem (Mb): 63.791015625\n",
      "Current step: 90  Loss: 0.3537302017211914  Acc: 0.7498327493667603  AllocMem (Mb): 63.791015625\n",
      "Current step: 100  Loss: 0.7618903517723083  Acc: 0.7034725546836853  AllocMem (Mb): 63.791015625\n",
      "Current step: 110  Loss: 0.323570191860199  Acc: 0.8562735319137573  AllocMem (Mb): 63.791015625\n",
      "Current step: 120  Loss: 0.3775845170021057  Acc: 0.823987603187561  AllocMem (Mb): 63.791015625\n",
      "Current step: 130  Loss: 0.29826509952545166  Acc: 0.8045241832733154  AllocMem (Mb): 63.791015625\n",
      "Current step: 140  Loss: 0.4658578932285309  Acc: 0.6810919046401978  AllocMem (Mb): 63.791015625\n",
      "Current step: 150  Loss: 0.274218887090683  Acc: 0.8728869557380676  AllocMem (Mb): 63.791015625\n",
      "Current step: 160  Loss: 0.24069499969482422  Acc: 0.8860676884651184  AllocMem (Mb): 63.791015625\n",
      "Current step: 170  Loss: 0.30008378624916077  Acc: 0.8239141702651978  AllocMem (Mb): 63.791015625\n",
      "Current step: 180  Loss: 0.3012883961200714  Acc: 0.8033628463745117  AllocMem (Mb): 63.791015625\n",
      "Current step: 190  Loss: 0.4483218789100647  Acc: 0.6366249322891235  AllocMem (Mb): 63.791015625\n",
      "Current step: 200  Loss: 0.35201358795166016  Acc: 0.9115080833435059  AllocMem (Mb): 63.791015625\n",
      "Current step: 210  Loss: 0.4132862687110901  Acc: 0.7152659296989441  AllocMem (Mb): 63.791015625\n",
      "Current step: 220  Loss: 0.37607982754707336  Acc: 0.6874915361404419  AllocMem (Mb): 63.791015625\n",
      "Current step: 230  Loss: 0.3332352042198181  Acc: 0.737474799156189  AllocMem (Mb): 63.791015625\n",
      "Current step: 240  Loss: 0.33118629455566406  Acc: 0.8754775524139404  AllocMem (Mb): 63.791015625\n",
      "Current step: 250  Loss: 0.2777947187423706  Acc: 0.8315486311912537  AllocMem (Mb): 63.791015625\n",
      "Current step: 260  Loss: 0.30453068017959595  Acc: 0.8881819248199463  AllocMem (Mb): 63.791015625\n",
      "Current step: 270  Loss: 0.3133731782436371  Acc: 0.8763750195503235  AllocMem (Mb): 63.791015625\n",
      "Current step: 280  Loss: 0.2995609641075134  Acc: 0.8116980791091919  AllocMem (Mb): 63.791015625\n",
      "Current step: 290  Loss: 0.23736754059791565  Acc: 0.9390383362770081  AllocMem (Mb): 63.791015625\n",
      "Current step: 300  Loss: 0.33643820881843567  Acc: 0.8170391917228699  AllocMem (Mb): 63.791015625\n",
      "Current step: 310  Loss: 0.2840135097503662  Acc: 0.8855675458908081  AllocMem (Mb): 63.791015625\n",
      "Current step: 320  Loss: 0.2693919837474823  Acc: 0.8997864723205566  AllocMem (Mb): 63.791015625\n",
      "Current step: 330  Loss: 0.20249786972999573  Acc: 0.9753468632698059  AllocMem (Mb): 63.791015625\n",
      "Current step: 340  Loss: 0.4004753530025482  Acc: 0.7886126637458801  AllocMem (Mb): 63.791015625\n",
      "Current step: 350  Loss: 0.3246555030345917  Acc: 0.7713916897773743  AllocMem (Mb): 63.791015625\n",
      "Current step: 360  Loss: 0.3178246021270752  Acc: 0.8510787487030029  AllocMem (Mb): 63.791015625\n",
      "Current step: 370  Loss: 0.23778121173381805  Acc: 0.913317084312439  AllocMem (Mb): 63.791015625\n",
      "Current step: 380  Loss: 0.15411458909511566  Acc: 0.9750908613204956  AllocMem (Mb): 63.791015625\n",
      "Current step: 390  Loss: 0.1908135712146759  Acc: 0.9299910068511963  AllocMem (Mb): 63.791015625\n",
      "Current step: 400  Loss: 0.2452213168144226  Acc: 0.9505581259727478  AllocMem (Mb): 63.791015625\n",
      "Current step: 410  Loss: 0.2663485109806061  Acc: 0.895337700843811  AllocMem (Mb): 63.791015625\n",
      "Current step: 420  Loss: 0.26467445492744446  Acc: 0.9431135654449463  AllocMem (Mb): 63.791015625\n",
      "Current step: 430  Loss: 0.35403475165367126  Acc: 0.7757670283317566  AllocMem (Mb): 63.791015625\n",
      "Current step: 440  Loss: 0.603373646736145  Acc: 0.6410138010978699  AllocMem (Mb): 63.791015625\n",
      "Current step: 450  Loss: 0.2759850323200226  Acc: 0.8311569690704346  AllocMem (Mb): 63.791015625\n",
      "Current step: 460  Loss: 0.3025496304035187  Acc: 0.8822795748710632  AllocMem (Mb): 63.791015625\n",
      "Current step: 470  Loss: 0.15305830538272858  Acc: 0.9785625338554382  AllocMem (Mb): 63.791015625\n",
      "Current step: 480  Loss: 0.2959024906158447  Acc: 0.8231054544448853  AllocMem (Mb): 63.791015625\n",
      "Current step: 490  Loss: 0.14879247546195984  Acc: 0.9169797897338867  AllocMem (Mb): 63.791015625\n",
      "Current step: 500  Loss: 0.2575708329677582  Acc: 0.8832584023475647  AllocMem (Mb): 63.791015625\n",
      "train Loss: 0.3546 Acc: 0.8130025863647461\n",
      "Current step: 10  Loss: 0.27351245284080505  Acc: 0.8338521122932434  AllocMem (Mb): 62.7919921875\n",
      "Current step: 20  Loss: 0.26377546787261963  Acc: 0.8793193697929382  AllocMem (Mb): 63.7919921875\n",
      "Current step: 30  Loss: 0.40684568881988525  Acc: 0.8432459235191345  AllocMem (Mb): 63.7919921875\n",
      "Current step: 40  Loss: 0.38641950488090515  Acc: 0.772264838218689  AllocMem (Mb): 62.7919921875\n",
      "Current step: 50  Loss: 0.29318538308143616  Acc: 0.8587324619293213  AllocMem (Mb): 63.7919921875\n",
      "Current step: 60  Loss: 0.3751369118690491  Acc: 0.8359748125076294  AllocMem (Mb): 63.7919921875\n",
      "Current step: 70  Loss: 0.3028181493282318  Acc: 0.9018673300743103  AllocMem (Mb): 62.7919921875\n",
      "Current step: 80  Loss: 0.28073886036872864  Acc: 0.9280706644058228  AllocMem (Mb): 63.7919921875\n",
      "Current step: 90  Loss: 0.3785020112991333  Acc: 0.8305991888046265  AllocMem (Mb): 63.7919921875\n",
      "Current step: 100  Loss: 0.241676464676857  Acc: 0.944989800453186  AllocMem (Mb): 62.7919921875\n",
      "Current step: 110  Loss: 0.24475495517253876  Acc: 0.9335813522338867  AllocMem (Mb): 63.7919921875\n",
      "Current step: 120  Loss: 0.2758808732032776  Acc: 0.9113204479217529  AllocMem (Mb): 63.7919921875\n",
      "Current step: 130  Loss: 0.20614956319332123  Acc: 0.8811464905738831  AllocMem (Mb): 62.7919921875\n",
      "Current step: 140  Loss: 0.372293621301651  Acc: 0.7473816871643066  AllocMem (Mb): 63.7919921875\n",
      "Current step: 150  Loss: 0.3960677683353424  Acc: 0.7517943382263184  AllocMem (Mb): 63.7919921875\n",
      "Current step: 160  Loss: 0.38051050901412964  Acc: 0.7673091292381287  AllocMem (Mb): 62.7919921875\n",
      "Current step: 170  Loss: 0.36491793394088745  Acc: 0.7283585071563721  AllocMem (Mb): 63.7919921875\n",
      "Current step: 180  Loss: 0.16447442770004272  Acc: 0.9392191767692566  AllocMem (Mb): 63.7919921875\n",
      "Current step: 190  Loss: 0.32781681418418884  Acc: 0.8357967734336853  AllocMem (Mb): 62.7919921875\n",
      "Current step: 200  Loss: 0.30853989720344543  Acc: 0.8258271217346191  AllocMem (Mb): 63.7919921875\n",
      "valid Loss: 0.3109 Acc: 0.8439164161682129\n",
      "Epoch 1/2\n",
      "----------\n",
      "Current step: 10  Loss: 0.14635665714740753  Acc: 0.9776820540428162  AllocMem (Mb): 63.79248046875\n",
      "Current step: 20  Loss: 0.2672508656978607  Acc: 0.8720946311950684  AllocMem (Mb): 63.79248046875\n",
      "Current step: 30  Loss: 0.24547405540943146  Acc: 0.9267696738243103  AllocMem (Mb): 63.79248046875\n",
      "Current step: 40  Loss: 0.20314647257328033  Acc: 0.9177161455154419  AllocMem (Mb): 63.79248046875\n",
      "Current step: 50  Loss: 0.3196002244949341  Acc: 0.8350033164024353  AllocMem (Mb): 63.79248046875\n",
      "Current step: 60  Loss: 0.19688312709331512  Acc: 0.9224808216094971  AllocMem (Mb): 63.79248046875\n",
      "Current step: 70  Loss: 0.271686315536499  Acc: 0.8509482145309448  AllocMem (Mb): 63.79248046875\n",
      "Current step: 80  Loss: 0.5028257369995117  Acc: 0.8044846057891846  AllocMem (Mb): 63.79248046875\n",
      "Current step: 90  Loss: 0.2942875325679779  Acc: 0.8895020484924316  AllocMem (Mb): 63.79248046875\n",
      "Current step: 100  Loss: 0.32320722937583923  Acc: 0.8382506370544434  AllocMem (Mb): 63.79248046875\n",
      "Current step: 110  Loss: 0.477189302444458  Acc: 0.7620391845703125  AllocMem (Mb): 63.79248046875\n",
      "Current step: 120  Loss: 0.26365962624549866  Acc: 0.9690252542495728  AllocMem (Mb): 63.79248046875\n",
      "Current step: 130  Loss: 0.31812790036201477  Acc: 0.8362087607383728  AllocMem (Mb): 63.79248046875\n",
      "Current step: 140  Loss: 0.12034263461828232  Acc: 0.9641158580780029  AllocMem (Mb): 63.79248046875\n",
      "Current step: 150  Loss: 0.15721043944358826  Acc: 0.9452164173126221  AllocMem (Mb): 63.79248046875\n",
      "Current step: 160  Loss: 0.29026955366134644  Acc: 0.7956656217575073  AllocMem (Mb): 63.79248046875\n",
      "Current step: 170  Loss: 0.29453444480895996  Acc: 0.8397968411445618  AllocMem (Mb): 63.79248046875\n",
      "Current step: 180  Loss: 0.1882302612066269  Acc: 0.9320040345191956  AllocMem (Mb): 63.79248046875\n",
      "Current step: 190  Loss: 0.24363800883293152  Acc: 0.9178709983825684  AllocMem (Mb): 63.79248046875\n",
      "Current step: 200  Loss: 0.28653374314308167  Acc: 0.8805592656135559  AllocMem (Mb): 63.79248046875\n",
      "Current step: 210  Loss: 0.21621300280094147  Acc: 0.9434577226638794  AllocMem (Mb): 63.79248046875\n",
      "Current step: 220  Loss: 0.2894558906555176  Acc: 0.891497015953064  AllocMem (Mb): 63.79248046875\n",
      "Current step: 230  Loss: 0.25700506567955017  Acc: 0.8958367109298706  AllocMem (Mb): 63.79248046875\n",
      "Current step: 240  Loss: 0.490246057510376  Acc: 0.8199462890625  AllocMem (Mb): 63.79248046875\n",
      "Current step: 250  Loss: 0.22606144845485687  Acc: 0.8947172164916992  AllocMem (Mb): 63.79248046875\n",
      "Current step: 260  Loss: 0.309612512588501  Acc: 0.8404586315155029  AllocMem (Mb): 63.79248046875\n",
      "Current step: 270  Loss: 0.24288509786128998  Acc: 0.9028806686401367  AllocMem (Mb): 63.79248046875\n",
      "Current step: 280  Loss: 0.17618022859096527  Acc: 0.9268092513084412  AllocMem (Mb): 63.79248046875\n",
      "Current step: 290  Loss: 0.29165777564048767  Acc: 0.8335384726524353  AllocMem (Mb): 63.79248046875\n",
      "Current step: 300  Loss: 0.3687126338481903  Acc: 0.8140987753868103  AllocMem (Mb): 63.79248046875\n",
      "Current step: 310  Loss: 0.1432092934846878  Acc: 0.9737792015075684  AllocMem (Mb): 63.79248046875\n",
      "Current step: 320  Loss: 0.16705472767353058  Acc: 0.9676101207733154  AllocMem (Mb): 63.79248046875\n",
      "Current step: 330  Loss: 0.24954067170619965  Acc: 0.9400595426559448  AllocMem (Mb): 63.79248046875\n",
      "Current step: 340  Loss: 0.4692385196685791  Acc: 0.7630230784416199  AllocMem (Mb): 63.79248046875\n",
      "Current step: 350  Loss: 0.22745083272457123  Acc: 0.9240310192108154  AllocMem (Mb): 63.79248046875\n",
      "Current step: 360  Loss: 0.293505996465683  Acc: 0.8456002473831177  AllocMem (Mb): 63.79248046875\n",
      "Current step: 370  Loss: 0.30103805661201477  Acc: 0.8882265686988831  AllocMem (Mb): 63.79248046875\n",
      "Current step: 380  Loss: 0.12236170470714569  Acc: 0.9645894169807434  AllocMem (Mb): 63.79248046875\n",
      "Current step: 390  Loss: 0.2104545682668686  Acc: 0.9521026611328125  AllocMem (Mb): 63.79248046875\n",
      "Current step: 400  Loss: 0.4502071440219879  Acc: 0.7751052379608154  AllocMem (Mb): 63.79248046875\n",
      "Current step: 410  Loss: 0.2695331275463104  Acc: 0.8727445602416992  AllocMem (Mb): 63.79248046875\n",
      "Current step: 420  Loss: 0.5543627738952637  Acc: 0.683916449546814  AllocMem (Mb): 63.79248046875\n",
      "Current step: 430  Loss: 0.13410110771656036  Acc: 0.9587611556053162  AllocMem (Mb): 63.79248046875\n",
      "Current step: 440  Loss: 0.31721070408821106  Acc: 0.8690214157104492  AllocMem (Mb): 63.79248046875\n",
      "Current step: 450  Loss: 0.2290087640285492  Acc: 0.9406698942184448  AllocMem (Mb): 63.79248046875\n",
      "Current step: 460  Loss: 0.3455999195575714  Acc: 0.8065394759178162  AllocMem (Mb): 63.79248046875\n",
      "Current step: 470  Loss: 0.338072270154953  Acc: 0.7244296669960022  AllocMem (Mb): 63.79248046875\n",
      "Current step: 480  Loss: 0.3564518690109253  Acc: 0.8488227128982544  AllocMem (Mb): 63.79248046875\n",
      "Current step: 490  Loss: 0.2729909420013428  Acc: 0.8466282486915588  AllocMem (Mb): 63.79248046875\n",
      "Current step: 500  Loss: 0.37279602885246277  Acc: 0.7706180214881897  AllocMem (Mb): 63.79248046875\n",
      "train Loss: 0.3015 Acc: 0.8548202514648438\n",
      "Current step: 10  Loss: 0.3371898829936981  Acc: 0.8698108792304993  AllocMem (Mb): 62.79296875\n",
      "Current step: 20  Loss: 0.2884432077407837  Acc: 0.8944278359413147  AllocMem (Mb): 63.79296875\n",
      "Current step: 30  Loss: 0.1665503829717636  Acc: 0.9374779462814331  AllocMem (Mb): 63.79296875\n",
      "Current step: 40  Loss: 0.166155144572258  Acc: 0.9676626920700073  AllocMem (Mb): 62.79296875\n",
      "Current step: 50  Loss: 0.32795754075050354  Acc: 0.7771075367927551  AllocMem (Mb): 63.79296875\n",
      "Current step: 60  Loss: 0.4059686064720154  Acc: 0.7539000511169434  AllocMem (Mb): 63.79296875\n",
      "Current step: 70  Loss: 0.37980106472969055  Acc: 0.8551076054573059  AllocMem (Mb): 62.79296875\n",
      "Current step: 80  Loss: 0.3006242513656616  Acc: 0.9009003639221191  AllocMem (Mb): 63.79296875\n",
      "Current step: 90  Loss: 0.3541710376739502  Acc: 0.8685839772224426  AllocMem (Mb): 63.79296875\n",
      "Current step: 100  Loss: 0.3656116724014282  Acc: 0.836595892906189  AllocMem (Mb): 62.79296875\n",
      "Current step: 110  Loss: 0.2849137485027313  Acc: 0.8898739218711853  AllocMem (Mb): 63.79296875\n",
      "Current step: 120  Loss: 0.15539038181304932  Acc: 0.9567272067070007  AllocMem (Mb): 63.79296875\n",
      "Current step: 130  Loss: 0.22076939046382904  Acc: 0.9654541015625  AllocMem (Mb): 62.79296875\n",
      "Current step: 140  Loss: 0.18039940297603607  Acc: 0.9727664589881897  AllocMem (Mb): 63.79296875\n",
      "Current step: 150  Loss: 0.34008872509002686  Acc: 0.8525718450546265  AllocMem (Mb): 63.79296875\n",
      "Current step: 160  Loss: 0.341002881526947  Acc: 0.8656887412071228  AllocMem (Mb): 62.79296875\n",
      "Current step: 170  Loss: 0.2135344296693802  Acc: 0.9321944713592529  AllocMem (Mb): 63.79296875\n",
      "Current step: 180  Loss: 0.2648434638977051  Acc: 0.8806971907615662  AllocMem (Mb): 63.79296875\n",
      "Current step: 190  Loss: 0.3672146201133728  Acc: 0.8008773326873779  AllocMem (Mb): 62.79296875\n",
      "Current step: 200  Loss: 0.3264053165912628  Acc: 0.9073989391326904  AllocMem (Mb): 63.79296875\n",
      "valid Loss: 0.2837 Acc: 0.8819896578788757\n",
      "Epoch 2/2\n",
      "----------\n",
      "Current step: 10  Loss: 0.2155883014202118  Acc: 0.9146101474761963  AllocMem (Mb): 63.79345703125\n",
      "Current step: 20  Loss: 0.29256489872932434  Acc: 0.8146938681602478  AllocMem (Mb): 63.79345703125\n",
      "Current step: 30  Loss: 0.24211767315864563  Acc: 0.9025245904922485  AllocMem (Mb): 63.79345703125\n",
      "Current step: 40  Loss: 0.3709753751754761  Acc: 0.8254321217536926  AllocMem (Mb): 63.79345703125\n",
      "Current step: 50  Loss: 0.2435247302055359  Acc: 0.9441002607345581  AllocMem (Mb): 63.79345703125\n",
      "Current step: 60  Loss: 0.10551858693361282  Acc: 0.9887633323669434  AllocMem (Mb): 63.79345703125\n",
      "Current step: 70  Loss: 0.26188838481903076  Acc: 0.9574477672576904  AllocMem (Mb): 63.79345703125\n",
      "Current step: 80  Loss: 0.22830705344676971  Acc: 0.9199015498161316  AllocMem (Mb): 63.79345703125\n",
      "Current step: 90  Loss: 0.1463572382926941  Acc: 0.9680594205856323  AllocMem (Mb): 63.79345703125\n",
      "Current step: 100  Loss: 0.27573832869529724  Acc: 0.8187363147735596  AllocMem (Mb): 63.79345703125\n",
      "Current step: 110  Loss: 0.376956582069397  Acc: 0.8067751526832581  AllocMem (Mb): 63.79345703125\n",
      "Current step: 120  Loss: 0.1519726663827896  Acc: 0.9566887617111206  AllocMem (Mb): 63.79345703125\n",
      "Current step: 130  Loss: 0.4461704194545746  Acc: 0.8483468294143677  AllocMem (Mb): 63.79345703125\n",
      "Current step: 140  Loss: 0.38927486538887024  Acc: 0.7730712890625  AllocMem (Mb): 63.79345703125\n",
      "Current step: 150  Loss: 0.3872387111186981  Acc: 0.8187244534492493  AllocMem (Mb): 63.79345703125\n",
      "Current step: 160  Loss: 0.47771531343460083  Acc: 0.8072543740272522  AllocMem (Mb): 63.79345703125\n",
      "Current step: 170  Loss: 0.28874829411506653  Acc: 0.8614909052848816  AllocMem (Mb): 63.79345703125\n",
      "Current step: 180  Loss: 0.21973009407520294  Acc: 0.8965120911598206  AllocMem (Mb): 63.79345703125\n",
      "Current step: 190  Loss: 0.36069291830062866  Acc: 0.7995142340660095  AllocMem (Mb): 63.79345703125\n",
      "Current step: 200  Loss: 0.2876738905906677  Acc: 0.8407841324806213  AllocMem (Mb): 63.79345703125\n",
      "Current step: 210  Loss: 0.15925389528274536  Acc: 0.9008823037147522  AllocMem (Mb): 63.79345703125\n",
      "Current step: 220  Loss: 0.24817393720149994  Acc: 0.8810379505157471  AllocMem (Mb): 63.79345703125\n",
      "Current step: 230  Loss: 0.1712574064731598  Acc: 0.9371982216835022  AllocMem (Mb): 63.79345703125\n",
      "Current step: 240  Loss: 0.17149600386619568  Acc: 0.9418075084686279  AllocMem (Mb): 63.79345703125\n",
      "Current step: 250  Loss: 0.18898171186447144  Acc: 0.8807836771011353  AllocMem (Mb): 63.79345703125\n",
      "Current step: 260  Loss: 0.276741623878479  Acc: 0.8377267122268677  AllocMem (Mb): 63.79345703125\n",
      "Current step: 270  Loss: 0.28362971544265747  Acc: 0.8371208906173706  AllocMem (Mb): 63.79345703125\n",
      "Current step: 280  Loss: 0.22732006013393402  Acc: 0.8815358281135559  AllocMem (Mb): 63.79345703125\n",
      "Current step: 290  Loss: 0.24118386209011078  Acc: 0.9147564768791199  AllocMem (Mb): 63.79345703125\n",
      "Current step: 300  Loss: 0.24448496103286743  Acc: 0.9287482500076294  AllocMem (Mb): 63.79345703125\n",
      "Current step: 310  Loss: 0.2730455696582794  Acc: 0.8879988193511963  AllocMem (Mb): 63.79345703125\n",
      "Current step: 320  Loss: 0.37404343485832214  Acc: 0.7684275507926941  AllocMem (Mb): 63.79345703125\n",
      "Current step: 330  Loss: 0.23573710024356842  Acc: 0.9204390048980713  AllocMem (Mb): 63.79345703125\n",
      "Current step: 340  Loss: 0.43802085518836975  Acc: 0.7671023011207581  AllocMem (Mb): 63.79345703125\n",
      "Current step: 350  Loss: 0.17028416693210602  Acc: 0.9246922135353088  AllocMem (Mb): 63.79345703125\n",
      "Current step: 360  Loss: 0.1996338665485382  Acc: 0.9661136269569397  AllocMem (Mb): 63.79345703125\n",
      "Current step: 370  Loss: 0.2382265031337738  Acc: 0.9125598073005676  AllocMem (Mb): 63.79345703125\n",
      "Current step: 380  Loss: 0.25208336114883423  Acc: 0.8497427701950073  AllocMem (Mb): 63.79345703125\n",
      "Current step: 390  Loss: 0.1935482770204544  Acc: 0.9070999622344971  AllocMem (Mb): 63.79345703125\n",
      "Current step: 400  Loss: 0.2149951308965683  Acc: 0.9105173945426941  AllocMem (Mb): 63.79345703125\n",
      "Current step: 410  Loss: 0.16219113767147064  Acc: 0.9142004251480103  AllocMem (Mb): 63.79345703125\n",
      "Current step: 420  Loss: 0.2102070152759552  Acc: 0.9118155241012573  AllocMem (Mb): 63.79345703125\n",
      "Current step: 430  Loss: 0.3879193067550659  Acc: 0.8398968577384949  AllocMem (Mb): 63.79345703125\n",
      "Current step: 440  Loss: 0.23727624118328094  Acc: 0.9070084095001221  AllocMem (Mb): 63.79345703125\n",
      "Current step: 450  Loss: 0.4281694293022156  Acc: 0.8332304954528809  AllocMem (Mb): 63.79345703125\n",
      "Current step: 460  Loss: 0.19817721843719482  Acc: 0.902269721031189  AllocMem (Mb): 63.79345703125\n",
      "Current step: 470  Loss: 0.21444056928157806  Acc: 0.9353666305541992  AllocMem (Mb): 63.79345703125\n",
      "Current step: 480  Loss: 0.24798443913459778  Acc: 0.8452131748199463  AllocMem (Mb): 63.79345703125\n",
      "Current step: 490  Loss: 0.23810940980911255  Acc: 0.9046986699104309  AllocMem (Mb): 63.79345703125\n",
      "Current step: 500  Loss: 0.4209412932395935  Acc: 0.723772406578064  AllocMem (Mb): 63.79345703125\n",
      "train Loss: 0.2812 Acc: 0.8649682402610779\n",
      "Current step: 10  Loss: 0.17348609864711761  Acc: 0.9611895680427551  AllocMem (Mb): 62.7939453125\n",
      "Current step: 20  Loss: 0.184087336063385  Acc: 0.9448389410972595  AllocMem (Mb): 63.7939453125\n",
      "Current step: 30  Loss: 0.37525317072868347  Acc: 0.7883594632148743  AllocMem (Mb): 63.7939453125\n",
      "Current step: 40  Loss: 0.23476499319076538  Acc: 0.8859264254570007  AllocMem (Mb): 62.7939453125\n",
      "Current step: 50  Loss: 0.19176028668880463  Acc: 0.9587413668632507  AllocMem (Mb): 63.7939453125\n",
      "Current step: 60  Loss: 0.35066625475883484  Acc: 0.8176693320274353  AllocMem (Mb): 63.7939453125\n",
      "Current step: 70  Loss: 0.2248135507106781  Acc: 0.9308714866638184  AllocMem (Mb): 62.7939453125\n",
      "Current step: 80  Loss: 0.426191121339798  Acc: 0.8295881748199463  AllocMem (Mb): 63.7939453125\n",
      "Current step: 90  Loss: 0.34732288122177124  Acc: 0.7810301780700684  AllocMem (Mb): 63.7939453125\n",
      "Current step: 100  Loss: 0.15147028863430023  Acc: 0.9841495156288147  AllocMem (Mb): 62.7939453125\n",
      "Current step: 110  Loss: 0.24791944026947021  Acc: 0.9336508512496948  AllocMem (Mb): 63.7939453125\n",
      "Current step: 120  Loss: 0.13884741067886353  Acc: 0.9560886025428772  AllocMem (Mb): 63.7939453125\n",
      "Current step: 130  Loss: 0.3596835434436798  Acc: 0.8694825172424316  AllocMem (Mb): 62.7939453125\n",
      "Current step: 140  Loss: 0.27592146396636963  Acc: 0.9139025807380676  AllocMem (Mb): 63.7939453125\n",
      "Current step: 150  Loss: 0.24644358456134796  Acc: 0.9494459629058838  AllocMem (Mb): 63.7939453125\n",
      "Current step: 160  Loss: 0.30840182304382324  Acc: 0.8688902854919434  AllocMem (Mb): 62.7939453125\n",
      "Current step: 170  Loss: 0.2077370285987854  Acc: 0.9478290677070618  AllocMem (Mb): 63.7939453125\n",
      "Current step: 180  Loss: 0.34271758794784546  Acc: 0.8737120628356934  AllocMem (Mb): 63.7939453125\n",
      "Current step: 190  Loss: 0.3541695773601532  Acc: 0.8196349143981934  AllocMem (Mb): 62.7939453125\n",
      "Current step: 200  Loss: 0.1756552755832672  Acc: 0.9353801608085632  AllocMem (Mb): 63.7939453125\n",
      "valid Loss: 0.2505 Acc: 0.901637613773346\n",
      "Training complete in 28m 8s\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(unet.parameters(), lr=0.01)\n",
    "train_loss, valid_loss = train(unet, train_dl, valid_dl, loss_fn, opt, acc_metric, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "interpreted-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet, 'testName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "outdoor-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = torch.load('38-Cloud-training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "broadband-award",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (upconv3): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv2): Sequential(\n",
       "    (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (upconv1): Sequential(\n",
       "    (0): Conv2d(64, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(2, 2, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-polymer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
